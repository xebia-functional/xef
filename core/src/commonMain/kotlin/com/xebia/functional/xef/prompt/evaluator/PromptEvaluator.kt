package com.xebia.functional.xef.prompt.evaluator

import com.xebia.functional.xef.auto.CoreAIScope
import com.xebia.functional.xef.auto.PromptConfiguration
import com.xebia.functional.xef.llm.Chat
import com.xebia.functional.xef.llm.models.chat.Message
import kotlinx.serialization.Serializable
import kotlinx.serialization.json.Json

object PromptEvaluator {

  @Serializable
  data class Score(
    val completeness: Double,
    val accuracy: Double,
    val efficiency: Double,
    val robustness: Double,
    val biasAndFairness: Double,
    val interpretability: Double,
    val usability: Double,
    val safetyAndSecurity: Double,
    val creativityAndFlexibility: Double,
    val internationalization: Double,
    val scalability: Double,
    val legalAndEthicalConsiderations: Double,
    val reasoning: String,
  )

  suspend fun evaluate(
    model: Chat,
    scope: CoreAIScope,
    prompt: String,
    response: String,
    promptConfiguration: PromptConfiguration = PromptConfiguration.DEFAULTS,
  ): Score {
    val result: List<String> = model.promptMessages(
      messages =
      listOf(
        Message.systemMessage {
          //language=markdown
          """
          # PromptEvaluator
          
          # Roleplay as an evaluator for testing the performance of prompts over LLMs.
          
          EvaluationProcess {
            State {
              Prompt: String
              Response: String
            }
            Completeness() {
              Coverage()
              Detail()
            }
            Accuracy() {
              Correctness()
              Consistency()
            }
            Efficiency() {
              ResponseTime()
              ResourceUtilization()
            }
            Robustness() {
              NoiseTolerance()
              Adaptability()
            }
            BiasAndFairness() {
              DemographicBias()
              ContentBias()
            }
            Interpretability() {
              Clarity()
              Explainability()
            }
            Usability() {
              UserExperience()
              Accessibility()
            }
            SafetyAndSecurity() {
              ContentSafety()
              DataSecurity()
            }
            CreativityAndFlexibility() {
              Innovation()
              Versatility()
            }
            Internationalization() {
              LanguageSupport()
              CulturalSensitivity()
            }
            Scalability() {
              VolumeHandling()
              Integration()
            }
            LegalAndEthicalConsiderations() {
              Compliance()
              EthicalConsiderations()
            }
            ScoreRules() {
              Apply a score to each evaluation.
              0 = not met
              0.25 = partially met but not enough
              0.5 = partially met
              0.75 = mostly met
              1 = met
            }
            Evaluate(prompt: String, response: String) : JSON {
              Set Prompt = prompt
              Set Response = response
              ScoreRules() {
                Completeness():evaluation=thorough
                Accuracy():evaluation=precise
                Efficiency():evaluation=optimized
                Robustness():evaluation=resilient
                BiasAndFairness():evaluation=unbiased
                Interpretability():evaluation=clear
                Usability():evaluation=user-friendly
                Safety():evaluation=safe
                Security():evaluation=secure
                Creativity():evaluation=innovative
                Internationalization():evaluation=global
                Scalability():evaluation=scalable
                LegalAndEthicalConsiderations():evaluation=compliant
              }
              return {
                completeness: {{infer:double}},
                accuracy: {{infer:double}},
                efficiency: {{infer:double}},
                robustness: {{infer:double}},
                biasAndFairness: {{infer:double}},
                interpretability: {{infer:double}},
                usability: {{infer:double}},
                safetyAndSecurity: {{infer:double}},
                creativityAndFlexibility: {{infer:double}},
                internationalization: {{infer:double}},
                scalability: {{infer:double}},
                legalAndEthicalConsiderations: {{infer:double}},
                reasoning: {{infer:string;short_answer;max_tokens=50}}
              }
            }:evaluation=complete,returns=json,stringformat=json_escaped
          }
          
          When asked to evaluate a prompt and its response, please carefully follow the
          instructions above and ensure that the evaluation is complete in all aspects. üìù
          Reply exclusively with the evaluation JSON object. üì§
          """.trimIndent()
        },
        Message.userMessage { "Set Prompt = $prompt" },
        Message.userMessage { "Set Response = $response" },
        Message.userMessage { "Evaluate(Prompt, Response)" }
      ),
      context = scope.context,
      conversationId = scope.conversationId,
      promptConfiguration = promptConfiguration,
    )
    val firstMessage = result.firstOrNull() ?: error("No messages returned from prompt")
    return Json.decodeFromString(Score.serializer(), firstMessage)
  }
}
